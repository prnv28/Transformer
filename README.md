# Neural Machine Translation using Transformer | English to Hindi

This repository contains an **implementation from scratch** of the Transformer architecture for **Neural Machine Translation (NMT)**. The model is trained to translate sentences from **English to Hindi**, based on the **IIT Bombay English-Hindi Parallel Corpus**.

> ğŸ“œ Paper Reference: "Attention is All You Need" by Vaswani et al. (2017)  
> ğŸ”— GitHub: [https://github.com/prnv28/Transformer](https://github.com/prnv28/Transformer)

---

## ğŸ“Œ Project Highlights

- Full implementation of the **Transformer model** using PyTorch
- Training on the **IITB English-Hindi parallel dataset**
- Complete preprocessing pipeline including tokenization, vocabulary creation, and padding
- Support for both **training and inference**
- Option to evaluate BLEU scores for model performance

---

## ğŸ—‚ï¸ Project Structure

```
â””â”€â”€ Transformer/
    â””â”€â”€ src
        â”œâ”€â”€ Inference.ipynb
        â”œâ”€â”€ config.py
        â”œâ”€â”€ dataset.py
        â”œâ”€â”€ model.py
        â”œâ”€â”€ requirements.txt
        â”œâ”€â”€ runs
        â”œâ”€â”€ tokenizer_en.json
        â”œâ”€â”€ tokenizer_hi.json
        â”œâ”€â”€ tokenizer_it.json
        â”œâ”€â”€ train.py
        â””â”€â”€ translate.py
```

---
## ğŸ› ï¸ Implementation Details

### Model Architecture
| Component              | Specification                          |
|------------------------|----------------------------------------|
| Embedding Dimension    | 512                                    |
| Feed Forward Dimension | 2048                                   |
| Attention Heads        | 8                                      |
| Encoder/Decoder Layers | 6                                      |
| Dropout                | 0.1                                    |

### Training
- Optimizer: Adam (Î²â‚=0.9, Î²â‚‚=0.98, Îµ=10â»â¹)
- Learning Rate: Custom schedule (warmup steps = 4000)
- Batch Size: 32
- Training epochs: 15
---

## ğŸ”§ Installation

1. **Clone the repository**
```bash
git clone https://github.com/prnv28/Transformer.git
cd Transformer
```

2. **Install required packages**
```bash
pip install -r requirements.txt
```
---

## ğŸš€ Training

To train the model from scratch:

```bash
python train.py
```

You can adjust hyperparameters like batch size, learning rate, and number of epochs inside `config.py`.


---

## ğŸ“š References

- Vaswani et al., "Attention is All You Need", NeurIPS 2017 [[paper](https://arxiv.org/abs/1706.03762)]
- IIT Bombay English-Hindi Parallel Corpus [[link](https://www.cfilt.iitb.ac.in/iitb_parallel/)]

---
